{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Convolution2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'tanh'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Convolution2D(64, (3, 3),  activation = 'tanh'))\n",
    "classifier.add(Convolution2D(64, (3, 3),  activation = 'tanh'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Convolution2D(128, (3, 3),  activation = 'tanh'))\n",
    "classifier.add(Convolution2D(128, (3, 3),  activation = 'tanh'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(activation = 'relu', units = 512))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(activation = 'relu', units = 256))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(activation = 'softmax', units = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16487 images belonging to 2 classes.\n",
      "Found 4123 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "        r'F:\\Users\\7\\Desktop\\dataset - sexo\\train',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        r'F:\\Users\\7\\Desktop\\dataset - sexo\\test',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "515/515 [==============================] - 33s 65ms/step - loss: 0.3929 - acc: 0.8192 - val_loss: 0.2689 - val_acc: 0.8896\n",
      "Epoch 2/30\n",
      "515/515 [==============================] - 31s 61ms/step - loss: 0.2573 - acc: 0.8974 - val_loss: 0.2842 - val_acc: 0.8927\n",
      "Epoch 3/30\n",
      "515/515 [==============================] - 31s 61ms/step - loss: 0.2275 - acc: 0.9094 - val_loss: 0.2145 - val_acc: 0.9176\n",
      "Epoch 4/30\n",
      "515/515 [==============================] - 31s 61ms/step - loss: 0.2005 - acc: 0.9216 - val_loss: 0.2289 - val_acc: 0.9127\n",
      "Epoch 5/30\n",
      "515/515 [==============================] - 31s 61ms/step - loss: 0.1897 - acc: 0.9286 - val_loss: 0.2218 - val_acc: 0.9240\n",
      "Epoch 6/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1835 - acc: 0.9309 - val_loss: 0.2484 - val_acc: 0.9005\n",
      "Epoch 7/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1739 - acc: 0.9348 - val_loss: 0.1969 - val_acc: 0.9237\n",
      "Epoch 8/30\n",
      "515/515 [==============================] - 31s 61ms/step - loss: 0.1649 - acc: 0.9412 - val_loss: 0.2088 - val_acc: 0.9220\n",
      "Epoch 9/30\n",
      "515/515 [==============================] - 31s 61ms/step - loss: 0.1689 - acc: 0.9396 - val_loss: 0.2483 - val_acc: 0.9008\n",
      "Epoch 10/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1682 - acc: 0.9367 - val_loss: 0.2126 - val_acc: 0.9228\n",
      "Epoch 11/30\n",
      "515/515 [==============================] - 31s 61ms/step - loss: 0.1631 - acc: 0.9420 - val_loss: 0.2061 - val_acc: 0.9242\n",
      "Epoch 12/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1536 - acc: 0.9431 - val_loss: 0.2224 - val_acc: 0.9206\n",
      "Epoch 13/30\n",
      "515/515 [==============================] - 31s 59ms/step - loss: 0.1479 - acc: 0.9470 - val_loss: 0.2126 - val_acc: 0.9259\n",
      "Epoch 14/30\n",
      "515/515 [==============================] - 31s 59ms/step - loss: 0.1470 - acc: 0.9498 - val_loss: 0.2090 - val_acc: 0.9220\n",
      "Epoch 15/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1439 - acc: 0.9497 - val_loss: 0.2009 - val_acc: 0.9272\n",
      "Epoch 16/30\n",
      "515/515 [==============================] - 31s 59ms/step - loss: 0.1414 - acc: 0.9500 - val_loss: 0.2155 - val_acc: 0.9303\n",
      "Epoch 17/30\n",
      "515/515 [==============================] - 30s 59ms/step - loss: 0.1388 - acc: 0.9502 - val_loss: 0.2531 - val_acc: 0.9034\n",
      "Epoch 18/30\n",
      "515/515 [==============================] - 31s 59ms/step - loss: 0.1442 - acc: 0.9505 - val_loss: 0.2057 - val_acc: 0.9340\n",
      "Epoch 19/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1311 - acc: 0.9536 - val_loss: 0.2432 - val_acc: 0.9049\n",
      "Epoch 20/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1405 - acc: 0.9511 - val_loss: 0.2393 - val_acc: 0.9210\n",
      "Epoch 21/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1338 - acc: 0.9552 - val_loss: 0.2127 - val_acc: 0.9210\n",
      "Epoch 22/30\n",
      "515/515 [==============================] - 32s 61ms/step - loss: 0.1374 - acc: 0.9527 - val_loss: 0.2080 - val_acc: 0.9328\n",
      "Epoch 23/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1297 - acc: 0.9555 - val_loss: 0.2235 - val_acc: 0.9240\n",
      "Epoch 24/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1231 - acc: 0.9569 - val_loss: 0.2362 - val_acc: 0.9232\n",
      "Epoch 25/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1274 - acc: 0.9558 - val_loss: 0.2266 - val_acc: 0.9250\n",
      "Epoch 26/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1252 - acc: 0.9569 - val_loss: 0.2067 - val_acc: 0.9230\n",
      "Epoch 27/30\n",
      "515/515 [==============================] - 31s 61ms/step - loss: 0.1245 - acc: 0.9562 - val_loss: 0.2226 - val_acc: 0.9252\n",
      "Epoch 28/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1229 - acc: 0.9585 - val_loss: 0.2367 - val_acc: 0.9220\n",
      "Epoch 29/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1165 - acc: 0.9592 - val_loss: 0.2296 - val_acc: 0.9264\n",
      "Epoch 30/30\n",
      "515/515 [==============================] - 31s 60ms/step - loss: 0.1144 - acc: 0.9609 - val_loss: 0.2408 - val_acc: 0.9188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b58226c5f8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch=16487//32,\n",
    "        epochs=30,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=4123//32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4123 images belonging to 2 classes.\n",
      "[[1777  158]\n",
      " [ 168 2020]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         afr       0.91      0.92      0.92      1935\n",
      "     afr-non       0.93      0.92      0.93      2188\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      4123\n",
      "   macro avg       0.92      0.92      0.92      4123\n",
      "weighted avg       0.92      0.92      0.92      4123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "predict_gen = test_datagen.flow_from_directory(directory=r'F:\\Users\\7\\Desktop\\dataset - sexo\\test',\n",
    "                                                              target_size=[64,64],\n",
    "                                                              batch_size=4123,\n",
    "                                                              class_mode='categorical')\n",
    "\n",
    "X_val_sample, res = next(predict_gen)\n",
    "y_pred = classifier.predict(X_val_sample)\n",
    "#classifier.evaluate_generator(test_datagen, steps=1, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=1)\n",
    "onedtrue = []\n",
    "onedpred = []\n",
    "for true, pred in zip(res, y_pred):\n",
    "    onedtrue.append(np.argmax(true))\n",
    "    onedpred.append(np.argmax(pred))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(onedtrue, onedpred))\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['afr', 'afr-non']\n",
    "print(classification_report(onedtrue, onedpred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('mf-92-basic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Sequential,Input,Model,load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, GaussianNoise\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 1757s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luido\\AppData\\Local\\Continuum\\anaconda3\\envs\\tg-gpu-ptchd\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", name=\"prediction\", units=2)`\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\luido\\AppData\\Local\\Continuum\\anaconda3\\envs\\tg-gpu-ptchd\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"pr...)`\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "vgg16 = VGG16(weights='imagenet')\n",
    "x = vgg16.get_layer('fc1').output\n",
    "x = BatchNormalization(name = 'BatchNormalization_0')(x)\n",
    "x = Dropout(0.3, name = 'dropout0')(x)\n",
    "x = vgg16.get_layer('fc2')(x)\n",
    "x = BatchNormalization(name = 'BatchNormalization_1')(x)\n",
    "x = Dropout(0.3, name = 'dropout1')(x)\n",
    "prediction = Dense(output_dim=2, activation='softmax', name='prediction')(x)\n",
    "model = Model(input=vgg16.input, output=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow0_col1 {\n",
       "            background-color:  red;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow1_col1 {\n",
       "            background-color:  red;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow2_col1 {\n",
       "            background-color:  red;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow3_col1 {\n",
       "            background-color:  red;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow4_col1 {\n",
       "            background-color:  red;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow5_col1 {\n",
       "            background-color:  red;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow6_col1 {\n",
       "            background-color:  red;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow7_col1 {\n",
       "            background-color:  red;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow8_col1 {\n",
       "            background-color:  red;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow9_col1 {\n",
       "            background-color:  red;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow10_col1 {\n",
       "            background-color:  red;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow11_col1 {\n",
       "            background-color:  red;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow12_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow13_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow14_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow15_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow16_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow17_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow18_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow19_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow20_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow21_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow22_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow23_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow24_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow25_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow26_col1 {\n",
       "            background-color:  yellow;\n",
       "        }</style>  \n",
       "<table id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2e\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >layer</th> \n",
       "        <th class=\"col_heading level0 col1\" >trainable</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row0\" class=\"row_heading level0 row0\" >0</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow0_col0\" class=\"data row0 col0\" >input_1</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow0_col1\" class=\"data row0 col1\" >False</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row1\" class=\"row_heading level0 row1\" >1</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow1_col0\" class=\"data row1 col0\" >block1_conv1</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow1_col1\" class=\"data row1 col1\" >False</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row2\" class=\"row_heading level0 row2\" >2</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow2_col0\" class=\"data row2 col0\" >block1_conv2</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow2_col1\" class=\"data row2 col1\" >False</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row3\" class=\"row_heading level0 row3\" >3</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow3_col0\" class=\"data row3 col0\" >block1_pool</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow3_col1\" class=\"data row3 col1\" >False</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row4\" class=\"row_heading level0 row4\" >4</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow4_col0\" class=\"data row4 col0\" >block2_conv1</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow4_col1\" class=\"data row4 col1\" >False</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row5\" class=\"row_heading level0 row5\" >5</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow5_col0\" class=\"data row5 col0\" >block2_conv2</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow5_col1\" class=\"data row5 col1\" >False</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row6\" class=\"row_heading level0 row6\" >6</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow6_col0\" class=\"data row6 col0\" >block2_pool</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow6_col1\" class=\"data row6 col1\" >False</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row7\" class=\"row_heading level0 row7\" >7</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow7_col0\" class=\"data row7 col0\" >block3_conv1</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow7_col1\" class=\"data row7 col1\" >False</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row8\" class=\"row_heading level0 row8\" >8</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow8_col0\" class=\"data row8 col0\" >block3_conv2</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow8_col1\" class=\"data row8 col1\" >False</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row9\" class=\"row_heading level0 row9\" >9</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow9_col0\" class=\"data row9 col0\" >block3_conv3</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow9_col1\" class=\"data row9 col1\" >False</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row10\" class=\"row_heading level0 row10\" >10</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow10_col0\" class=\"data row10 col0\" >block3_pool</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow10_col1\" class=\"data row10 col1\" >False</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row11\" class=\"row_heading level0 row11\" >11</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow11_col0\" class=\"data row11 col0\" >block4_conv1</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow11_col1\" class=\"data row11 col1\" >False</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row12\" class=\"row_heading level0 row12\" >12</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow12_col0\" class=\"data row12 col0\" >block4_conv2</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow12_col1\" class=\"data row12 col1\" >True</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row13\" class=\"row_heading level0 row13\" >13</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow13_col0\" class=\"data row13 col0\" >block4_conv3</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow13_col1\" class=\"data row13 col1\" >True</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row14\" class=\"row_heading level0 row14\" >14</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow14_col0\" class=\"data row14 col0\" >block4_pool</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow14_col1\" class=\"data row14 col1\" >True</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row15\" class=\"row_heading level0 row15\" >15</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow15_col0\" class=\"data row15 col0\" >block5_conv1</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow15_col1\" class=\"data row15 col1\" >True</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row16\" class=\"row_heading level0 row16\" >16</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow16_col0\" class=\"data row16 col0\" >block5_conv2</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow16_col1\" class=\"data row16 col1\" >True</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row17\" class=\"row_heading level0 row17\" >17</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow17_col0\" class=\"data row17 col0\" >block5_conv3</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow17_col1\" class=\"data row17 col1\" >True</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row18\" class=\"row_heading level0 row18\" >18</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow18_col0\" class=\"data row18 col0\" >block5_pool</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow18_col1\" class=\"data row18 col1\" >True</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row19\" class=\"row_heading level0 row19\" >19</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow19_col0\" class=\"data row19 col0\" >flatten</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow19_col1\" class=\"data row19 col1\" >True</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row20\" class=\"row_heading level0 row20\" >20</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow20_col0\" class=\"data row20 col0\" >fc1</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow20_col1\" class=\"data row20 col1\" >True</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row21\" class=\"row_heading level0 row21\" >21</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow21_col0\" class=\"data row21 col0\" >BatchNormalization_0</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow21_col1\" class=\"data row21 col1\" >True</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row22\" class=\"row_heading level0 row22\" >22</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow22_col0\" class=\"data row22 col0\" >dropout0</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow22_col1\" class=\"data row22 col1\" >True</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row23\" class=\"row_heading level0 row23\" >23</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow23_col0\" class=\"data row23 col0\" >fc2</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow23_col1\" class=\"data row23 col1\" >True</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row24\" class=\"row_heading level0 row24\" >24</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow24_col0\" class=\"data row24 col0\" >BatchNormalization_1</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow24_col1\" class=\"data row24 col1\" >True</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row25\" class=\"row_heading level0 row25\" >25</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow25_col0\" class=\"data row25 col0\" >dropout1</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow25_col1\" class=\"data row25 col1\" >True</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2elevel0_row26\" class=\"row_heading level0 row26\" >26</th> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow26_col0\" class=\"data row26 col0\" >prediction</td> \n",
       "        <td id=\"T_79ccc6f8_136c_11e9_8076_1c1b0dc0dc2erow26_col1\" class=\"data row26 col1\" >True</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x208b3ab2048>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "for layer in model.layers:\n",
    "    if layer.name in ['fc1', 'fc2', 'prediction', 'dropout0', 'dropout1', 'flatten',\n",
    "                      'block5_pool', 'block5_conv3', 'block5_conv2', 'block5_conv1',\n",
    "                      'block4_pool', 'block4_conv3', 'block4_conv2',\n",
    "                      'BatchNormalization_0', 'BatchNormalization_1']:\n",
    "        continue\n",
    "    layer.trainable = False\n",
    "\n",
    "df = pd.DataFrame(([layer.name, layer.trainable] for layer in model.layers), columns=['layer', 'trainable'])\n",
    "df.style.applymap(lambda trainable: f'background-color: {\"yellow\" if trainable else \"red\"}', subset=['trainable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "sgd = SGD(lr=1e-4, momentum=0.9)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16487 images belonging to 2 classes.\n",
      "Found 4123 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "\n",
    "def preprocess_input_vgg(x):\n",
    "\n",
    "    from keras.applications.vgg16 import preprocess_input\n",
    "    X = np.expand_dims(x, axis=0)\n",
    "    X = preprocess_input(X)\n",
    "    return X[0]\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(directory=r'F:\\Users\\7\\Desktop\\dataset - sexo\\train',\n",
    "                                                    target_size=[224,224],\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg)\n",
    "validation_generator = validation_datagen.flow_from_directory(directory=r'F:\\Users\\7\\Desktop\\dataset - sexo\\test',\n",
    "                                                              target_size=[224,224],\n",
    "                                                              batch_size=32,\n",
    "                                                              class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luido\\AppData\\Local\\Continuum\\anaconda3\\envs\\tg-gpu-ptchd\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \"\"\"\n",
      "C:\\Users\\luido\\AppData\\Local\\Continuum\\anaconda3\\envs\\tg-gpu-ptchd\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=515, validation_data=<keras_pre..., validation_steps=128, epochs=20)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "515/515 [==============================] - 237s 461ms/step - loss: 0.4576 - acc: 0.8214 - val_loss: 0.2299 - val_acc: 0.9194\n",
      "Epoch 2/20\n",
      "515/515 [==============================] - 234s 455ms/step - loss: 0.3189 - acc: 0.8813 - val_loss: 0.2446 - val_acc: 0.9164\n",
      "Epoch 3/20\n",
      "515/515 [==============================] - 233s 452ms/step - loss: 0.2681 - acc: 0.9027 - val_loss: 0.3401 - val_acc: 0.8829\n",
      "Epoch 4/20\n",
      "515/515 [==============================] - 234s 455ms/step - loss: 0.2279 - acc: 0.9167 - val_loss: 0.3170 - val_acc: 0.8866\n",
      "Epoch 5/20\n",
      "515/515 [==============================] - 234s 454ms/step - loss: 0.2197 - acc: 0.9194 - val_loss: 0.1971 - val_acc: 0.9374\n",
      "Epoch 6/20\n",
      "515/515 [==============================] - 238s 461ms/step - loss: 0.1913 - acc: 0.9301 - val_loss: 0.1971 - val_acc: 0.9279\n",
      "Epoch 7/20\n",
      "515/515 [==============================] - 230s 446ms/step - loss: 0.1814 - acc: 0.9328 - val_loss: 0.2027 - val_acc: 0.9298\n",
      "Epoch 8/20\n",
      "515/515 [==============================] - 216s 419ms/step - loss: 0.1623 - acc: 0.9431 - val_loss: 0.1990 - val_acc: 0.9340\n",
      "Epoch 9/20\n",
      "515/515 [==============================] - 210s 408ms/step - loss: 0.1494 - acc: 0.9453 - val_loss: 0.1816 - val_acc: 0.9394\n",
      "Epoch 10/20\n",
      "515/515 [==============================] - 210s 407ms/step - loss: 0.1397 - acc: 0.9508 - val_loss: 0.1849 - val_acc: 0.9367\n",
      "Epoch 11/20\n",
      "515/515 [==============================] - 226s 439ms/step - loss: 0.1333 - acc: 0.9521 - val_loss: 0.1781 - val_acc: 0.9377\n",
      "Epoch 12/20\n",
      "515/515 [==============================] - 236s 458ms/step - loss: 0.1291 - acc: 0.9523 - val_loss: 0.2667 - val_acc: 0.9061\n",
      "Epoch 13/20\n",
      "515/515 [==============================] - 242s 469ms/step - loss: 0.1145 - acc: 0.9601 - val_loss: 0.2236 - val_acc: 0.9203\n",
      "Epoch 14/20\n",
      "515/515 [==============================] - 226s 438ms/step - loss: 0.1159 - acc: 0.9593 - val_loss: 0.1821 - val_acc: 0.9406\n",
      "Epoch 15/20\n",
      "515/515 [==============================] - 212s 412ms/step - loss: 0.1023 - acc: 0.9648 - val_loss: 0.1729 - val_acc: 0.9443\n",
      "Epoch 16/20\n",
      "515/515 [==============================] - 212s 412ms/step - loss: 0.1012 - acc: 0.9649 - val_loss: 0.1886 - val_acc: 0.9438\n",
      "Epoch 17/20\n",
      "515/515 [==============================] - 212s 412ms/step - loss: 0.0922 - acc: 0.9665 - val_loss: 0.2043 - val_acc: 0.9330\n",
      "Epoch 18/20\n",
      "515/515 [==============================] - 212s 412ms/step - loss: 0.0951 - acc: 0.9682 - val_loss: 0.1849 - val_acc: 0.9435\n",
      "Epoch 19/20\n",
      "515/515 [==============================] - 212s 412ms/step - loss: 0.0955 - acc: 0.9681 - val_loss: 0.2277 - val_acc: 0.9301\n",
      "Epoch 20/20\n",
      "515/515 [==============================] - 212s 411ms/step - loss: 0.0936 - acc: 0.9670 - val_loss: 0.2730 - val_acc: 0.9201\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=16487//32,\n",
    "                    nb_epoch=20,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=4123//32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4123 images belonging to 2 classes.\n",
      "[[1672  263]\n",
      " [  59 2129]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         fem       0.97      0.86      0.91      1935\n",
      "        masc       0.89      0.97      0.93      2188\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      4123\n",
      "   macro avg       0.93      0.92      0.92      4123\n",
      "weighted avg       0.93      0.92      0.92      4123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "predict_gen = validation_datagen.flow_from_directory(directory=r'F:\\Users\\7\\Desktop\\dataset - sexo\\test',\n",
    "                                                              target_size=[224,224],\n",
    "                                                              batch_size=4123,\n",
    "                                                              class_mode='categorical')\n",
    "\n",
    "X_val_sample, res = next(predict_gen)\n",
    "y_pred = model.predict(X_val_sample)\n",
    "#model.evaluate_generator(validation_datagen, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=1)\n",
    "onedtrue = []\n",
    "onedpred = []\n",
    "for true, pred in zip(res, y_pred):\n",
    "    onedtrue.append(np.argmax(true))\n",
    "    onedpred.append(np.argmax(pred))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(onedtrue, onedpred))\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['fem', 'masc']\n",
    "print(classification_report(onedtrue, onedpred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mf-92-vgg16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Sequential,Input,Model,load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, GaussianNoise\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16487 images belonging to 2 classes.\n",
      "Found 4123 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "\n",
    "def preprocess_input_icpt(x):\n",
    "\n",
    "    from keras.applications.inception_v3 import preprocess_input\n",
    "    X = np.expand_dims(x, axis=0)\n",
    "    X = preprocess_input(X)\n",
    "    return X[0]\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_icpt,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(directory=r'F:\\Users\\7\\Desktop\\dataset - sexo\\train',\n",
    "                                                    target_size=[140, 140],\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_icpt)\n",
    "validation_generator = validation_datagen.flow_from_directory(directory=r'F:\\Users\\7\\Desktop\\dataset - sexo\\test',\n",
    "                                                              target_size=[140, 140],\n",
    "                                                              batch_size=32,\n",
    "                                                              class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luido\\AppData\\Local\\Continuum\\anaconda3\\envs\\tg-gpu-ptchd\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\luido\\AppData\\Local\\Continuum\\anaconda3\\envs\\tg-gpu-ptchd\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=64, validation_data=<keras_pre..., validation_steps=32, epochs=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "64/64 [==============================] - 21s 324ms/step - loss: 1.1451 - acc: 0.5610 - val_loss: 0.5827 - val_acc: 0.6953\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import GlobalAveragePooling2D, Dense\n",
    "from keras.models import Model\n",
    "inc_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# get layers and add average pooling layer\n",
    "x = inc_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# add fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "# add output layer\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inc_model.input, outputs=predictions)\n",
    "\n",
    "# freeze pre-trained model area's layer\n",
    "for layer in inc_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# update the weight that are added\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'] )\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=64,\n",
    "                    nb_epoch=1,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=32)\n",
    "\n",
    "# choose the layers which are updated by training\n",
    "#layer_num = len(model.layers)\n",
    "#for layer in model.layers[:248]:\n",
    "#    layer.trainable = False\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# training\n",
    "model.compile(optimizer=SGD(lr=1e-4, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luido\\AppData\\Local\\Continuum\\anaconda3\\envs\\tg-gpu-ptchd\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "C:\\Users\\luido\\AppData\\Local\\Continuum\\anaconda3\\envs\\tg-gpu-ptchd\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=515, shuffle=True, validation_data=<keras_pre..., validation_steps=128, epochs=30)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "515/515 [==============================] - 115s 224ms/step - loss: 0.1438 - acc: 0.9470 - val_loss: 0.1859 - val_acc: 0.9336\n",
      "Epoch 2/30\n",
      "515/515 [==============================] - 114s 222ms/step - loss: 0.1427 - acc: 0.9489 - val_loss: 0.1861 - val_acc: 0.9357\n",
      "Epoch 3/30\n",
      "515/515 [==============================] - 114s 221ms/step - loss: 0.1415 - acc: 0.9495 - val_loss: 0.1846 - val_acc: 0.9328\n",
      "Epoch 4/30\n",
      "515/515 [==============================] - 121s 236ms/step - loss: 0.1391 - acc: 0.9503 - val_loss: 0.1845 - val_acc: 0.9345\n",
      "Epoch 5/30\n",
      "515/515 [==============================] - 118s 228ms/step - loss: 0.1366 - acc: 0.9506 - val_loss: 0.1775 - val_acc: 0.9389\n",
      "Epoch 6/30\n",
      "515/515 [==============================] - 114s 221ms/step - loss: 0.1313 - acc: 0.9534 - val_loss: 0.1840 - val_acc: 0.9396\n",
      "Epoch 7/30\n",
      "515/515 [==============================] - 112s 218ms/step - loss: 0.1323 - acc: 0.9529 - val_loss: 0.1820 - val_acc: 0.9360\n",
      "Epoch 8/30\n",
      "515/515 [==============================] - 118s 229ms/step - loss: 0.1288 - acc: 0.9553 - val_loss: 0.1758 - val_acc: 0.9408\n",
      "Epoch 9/30\n",
      "515/515 [==============================] - 116s 226ms/step - loss: 0.1264 - acc: 0.9538 - val_loss: 0.1889 - val_acc: 0.9340\n",
      "Epoch 10/30\n",
      "515/515 [==============================] - 118s 229ms/step - loss: 0.1210 - acc: 0.9552 - val_loss: 0.1943 - val_acc: 0.9372\n",
      "Epoch 11/30\n",
      "515/515 [==============================] - 126s 245ms/step - loss: 0.1217 - acc: 0.9555 - val_loss: 0.1866 - val_acc: 0.9364\n",
      "Epoch 12/30\n",
      "515/515 [==============================] - 123s 240ms/step - loss: 0.1239 - acc: 0.9542 - val_loss: 0.1850 - val_acc: 0.9384\n",
      "Epoch 13/30\n",
      "515/515 [==============================] - 118s 230ms/step - loss: 0.1198 - acc: 0.9576 - val_loss: 0.1846 - val_acc: 0.9413\n",
      "Epoch 14/30\n",
      "515/515 [==============================] - 123s 240ms/step - loss: 0.1167 - acc: 0.9601 - val_loss: 0.1795 - val_acc: 0.9416\n",
      "Epoch 15/30\n",
      "515/515 [==============================] - 128s 249ms/step - loss: 0.1093 - acc: 0.9617 - val_loss: 0.1889 - val_acc: 0.9421\n",
      "Epoch 16/30\n",
      "515/515 [==============================] - 129s 250ms/step - loss: 0.1112 - acc: 0.9612 - val_loss: 0.1825 - val_acc: 0.9406\n",
      "Epoch 17/30\n",
      "515/515 [==============================] - 130s 252ms/step - loss: 0.1058 - acc: 0.9621 - val_loss: 0.1909 - val_acc: 0.9362\n",
      "Epoch 18/30\n",
      "515/515 [==============================] - 126s 246ms/step - loss: 0.1073 - acc: 0.9630 - val_loss: 0.1932 - val_acc: 0.9364\n",
      "Epoch 19/30\n",
      "515/515 [==============================] - 128s 248ms/step - loss: 0.1109 - acc: 0.9603 - val_loss: 0.1851 - val_acc: 0.9389\n",
      "Epoch 20/30\n",
      "515/515 [==============================] - 123s 238ms/step - loss: 0.1004 - acc: 0.9649 - val_loss: 0.1878 - val_acc: 0.9418\n",
      "Epoch 21/30\n",
      "515/515 [==============================] - 127s 247ms/step - loss: 0.1004 - acc: 0.9663 - val_loss: 0.1860 - val_acc: 0.9416\n",
      "Epoch 22/30\n",
      "515/515 [==============================] - 127s 246ms/step - loss: 0.0932 - acc: 0.9679 - val_loss: 0.1997 - val_acc: 0.9374\n",
      "Epoch 23/30\n",
      "515/515 [==============================] - 127s 247ms/step - loss: 0.0995 - acc: 0.9647 - val_loss: 0.2008 - val_acc: 0.9369\n",
      "Epoch 24/30\n",
      "515/515 [==============================] - 128s 249ms/step - loss: 0.0963 - acc: 0.9674 - val_loss: 0.1845 - val_acc: 0.9411\n",
      "Epoch 25/30\n",
      "515/515 [==============================] - 126s 245ms/step - loss: 0.0942 - acc: 0.9669 - val_loss: 0.1965 - val_acc: 0.9411\n",
      "Epoch 26/30\n",
      "515/515 [==============================] - 129s 251ms/step - loss: 0.0877 - acc: 0.9702 - val_loss: 0.1884 - val_acc: 0.9377\n",
      "Epoch 27/30\n",
      "515/515 [==============================] - 121s 235ms/step - loss: 0.0899 - acc: 0.9683 - val_loss: 0.1880 - val_acc: 0.9418\n",
      "Epoch 28/30\n",
      "515/515 [==============================] - 119s 231ms/step - loss: 0.0887 - acc: 0.9704 - val_loss: 0.1975 - val_acc: 0.9379\n",
      "Epoch 29/30\n",
      "515/515 [==============================] - 117s 227ms/step - loss: 0.0933 - acc: 0.9682 - val_loss: 0.1923 - val_acc: 0.9408\n",
      "Epoch 30/30\n",
      "515/515 [==============================] - 119s 230ms/step - loss: 0.0821 - acc: 0.9723 - val_loss: 0.1919 - val_acc: 0.9433\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=16487//32,\n",
    "                    nb_epoch=30,\n",
    "                    shuffle = True,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=4123//32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4123 images belonging to 2 classes.\n",
      "[[1825  110]\n",
      " [ 132 2056]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         fem       0.93      0.94      0.94      1935\n",
      "        masc       0.95      0.94      0.94      2188\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      4123\n",
      "   macro avg       0.94      0.94      0.94      4123\n",
      "weighted avg       0.94      0.94      0.94      4123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "predict_gen = validation_datagen.flow_from_directory(directory=r'F:\\Users\\7\\Desktop\\dataset - sexo\\test',\n",
    "                                                              target_size=[140,140],\n",
    "                                                              batch_size=4123,\n",
    "                                                              class_mode='categorical')\n",
    "\n",
    "X_val_sample, res = next(predict_gen)\n",
    "y_pred = model.predict(X_val_sample)\n",
    "#model.evaluate_generator(validation_datagen, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=1)\n",
    "onedtrue = []\n",
    "onedpred = []\n",
    "for true, pred in zip(res, y_pred):\n",
    "    onedtrue.append(np.argmax(true))\n",
    "    onedpred.append(np.argmax(pred))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(onedtrue, onedpred))\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['fem', 'masc']\n",
    "print(classification_report(onedtrue, onedpred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mf-94-incept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-patched",
   "language": "python",
   "name": "tf-gpu-patched"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
